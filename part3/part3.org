* PART 3
- https://www.udacity.com/course/machine-learning-for-trading--ud501
** 03-01 - How Machine Learning is used at a hedge fund
Building models
*** The ML problem

x -> Model -> y

Observation -> Model -> Prediction

Data -> ML -> Model

*** What's X and Y

Future price or return are predictions

*** Supervised Regression Learning

Supervised
Provide examples of x and y

Regression
Numerical approximation or prediction

Learning
Training with data


- Linear regression (parametric)
  - Finds the parameters for a model

- K nearest neighbor (KNN) (instance based)
  - Instance based because the data is kept from the training and consulted

- Decision trees
  - Query goes through the tree. Each node is a question. Reach a leave which is the regression value which is returned

- Decision forests
  - Collections of trees

*** Robot navigation example

video

*** How it works with stock data

Go back into the historical data
Pick a Y and then an X which is say 5 days prior
Then move forward and capture each instance or data point and save

This can be your training data

X's can be muli-dimensional
They are called factors

*** Example at a fintech company

Lucena Research

- Select X1, S2, X3, ... predictive factors
- Select Y. What you are interested in predicting
- Time period, stock universe
- Train
  - Takes data and creates a model

*** Price forcasting demo

QuantDesk
Cloud-based application created by Lucena Research

Select Forecasting Options

Select Model
Model defaults -> Factors

Forecast -> 1 week
Lookback -> 3 months

*** Backtesting

Roll back time and test on historical data
Limit data to previous data
and see how the system performs predicting the next data

*** ML tool in use

Example from QuantDesk

*** Problems with regresssion

Regression based forecasing can be useful

Thoughs issues:

- Nosiy and uncertain
- Challenging to estimate confidence
- Holding time, allocation. How to decide


Another idea:
Policy learning / Reenforcement Learning
more later on this

*** Problem we will focus on

Period - 2009

Text over the period - 2010 and 2011

Generate orders.txt and push through your market simulator

** 03-02 - Regression
*** Introduction
Supervised Regresion Learnning

Numerical Model

*** Parametric Regression

Building a model that is built with a number of parameters

y = mx + b

m and b are parameters

linear regression, fitting data to a line

Can fit a polynomial to better fit
y = m2 * x^2 + mx + b

Can add more terms. x^3, etc

Data is used to build a model (formula)

*** K nearest neighbor

Data centric or Instance approach

Look at the data for nearest data points to answer a query

*** How to predict

Since we are trying to predict y (rain) at given x (change in pressure), using the mean of nearest observed y values makes sens

*** Kernel Regression

You can repeat the process for all of the data points and you'll have a fitted line

Methods:
- K nearest neighbor (KNN)
  - non-weighted

- Kernel Regression
  - weighted


Instance based, keep the data and it is consulted when you make a query

*** Quiz: Parametric vs non?

Yes, the cannon ball distance can be best estimated using a parametric model, as it follows a well-defined trajectory.

On the other hand, the behavior of honey bees can be hard to model mathematically. Therefore, a non-parametric approach would be more suitable.

If you can quess at an equation a parametric model might work. If you don't or the data doesn't appear to be equation based, try a non-parametric

Parameteric doesn't have to store the data but you need it to re-train or add data.
Training is slow, query is fast

Non-Parameteric
You need to store the data and adding data is eadiy
Training is fast but querying can be slow

*** Training and Testing

Features
Different indicators
Multiple features

Prices, output are Y

Training and Testing data need to be segmented apart

Out of sample testing

|----------+--------|
| Features | Prices |
|----------+--------|
|          |        |
| XTrain   | YTrain |
|----------+--------|
|          |        |
| XTest    | YTest  |
|----------+--------|


Train on older data and test on new data

*** Learning APIs

For linear regression:

learner = LinRegLearner()

learner.train(Xtrain, Ytrain)

y = learner.query(Xtest)

These y values will be compared with Ytest values

For KNN:

learner = KNNLearner(k=3)

learner.train(Xtrain, Ytrain)

y = learner.query(Xtest)

*** Example for linear regression

#+BEGIN_SRC python

class LinRegLearner::

    def __init__():
        pass

    def train(X, Y):
        # find m and b for y = mx + b
        self.m, self.b = favorite_linreg(X, Y)         # see scipy, numpy

    def query(X):
        y = self.m * X + self.b
        return y

#+END_SRC

- https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.linregress.html
- scipy.stats.linregress


- KNN should have the same interface so you can try either type easily


- http://www.kdnuggets.com/2016/01/implementing-your-own-knn-using-python.html
- https://ashokharnal.wordpress.com/tag/k-nearest-neighbor-classification-example-using-python/
- ...

** 03-03 - Assessing a learning algorithm

*** Overview

There are many algorithms other than linear regression and knn

*** A closer look at KNN solutions

If you plot results the line is ragged.

Model can't extrapolate
Horizontal lines before and after the data

*** 
