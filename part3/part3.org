* PART 3
- https://www.udacity.com/course/machine-learning-for-trading--ud501
** 03-01 - How Machine Learning is used at a hedge fund
Building models
*** The ML problem

x -> Model -> y

Observation -> Model -> Prediction

Data -> ML -> Model

*** What's X and Y

Future price or return are predictions

*** Supervised Regression Learning

Supervised
Provide examples of x and y

Regression
Numerical approximation or prediction

Learning
Training with data


- Linear regression (parametric)
  - Finds the parameters for a model

- K nearest neighbor (KNN) (instance based)
  - Instance based because the data is kept from the training and consulted

- Decision trees
  - Query goes through the tree. Each node is a question. Reach a leave which is the regression value which is returned

- Decision forests
  - Collections of trees

*** Robot navigation example

video

*** How it works with stock data

Go back into the historical data
Pick a Y and then an X which is say 5 days prior
Then move forward and capture each instance or data point and save

This can be your training data

X's can be muli-dimensional
They are called factors

*** Example at a fintech company

Lucena Research

- Select X1, S2, X3, ... predictive factors
- Select Y. What you are interested in predicting
- Time period, stock universe
- Train
  - Takes data and creates a model

*** Price forcasting demo

QuantDesk
Cloud-based application created by Lucena Research

Select Forecasting Options

Select Model
Model defaults -> Factors

Forecast -> 1 week
Lookback -> 3 months

*** Backtesting

Roll back time and test on historical data
Limit data to previous data
and see how the system performs predicting the next data

*** ML tool in use

Example from QuantDesk

*** Problems with regresssion

Regression based forecasing can be useful

Thoughs issues:

- Nosiy and uncertain
- Challenging to estimate confidence
- Holding time, allocation. How to decide


Another idea:
Policy learning / Reenforcement Learning
more later on this

*** Problem we will focus on

Period - 2009

Text over the period - 2010 and 2011

Generate orders.txt and push through your market simulator

** 03-02 - Regression
*** Introduction
Supervised Regresion Learnning

Numerical Model

*** Parametric Regression

Building a model that is built with a number of parameters

y = mx + b

m and b are parameters

linear regression, fitting data to a line

Can fit a polynomial to better fit
y = m2 * x^2 + mx + b

Can add more terms. x^3, etc

Data is used to build a model (formula)

*** K nearest neighbor

Data centric or Instance approach

Look at the data for nearest data points to answer a query

*** How to predict

Since we are trying to predict y (rain) at given x (change in pressure), using the mean of nearest observed y values makes sens

*** Kernel Regression

You can repeat the process for all of the data points and you'll have a fitted line

Methods:
- K nearest neighbor (KNN)
  - non-weighted

- Kernel Regression
  - weighted


Instance based, keep the data and it is consulted when you make a query

*** Quiz: Parametric vs non?

Yes, the cannon ball distance can be best estimated using a parametric model, as it follows a well-defined trajectory.

On the other hand, the behavior of honey bees can be hard to model mathematically. Therefore, a non-parametric approach would be more suitable.

If you can quess at an equation a parametric model might work. If you don't or the data doesn't appear to be equation based, try a non-parametric

Parameteric doesn't have to store the data but you need it to re-train or add data.
Training is slow, query is fast

Non-Parameteric
You need to store the data and adding data is eadiy
Training is fast but querying can be slow

*** Training and Testing

Features
Different indicators
Multiple features

Prices, output are Y

Training and Testing data need to be segmented apart

Out of sample testing

|----------+--------|
| Features | Prices |
|----------+--------|
|          |        |
| XTrain   | YTrain |
|----------+--------|
|          |        |
| XTest    | YTest  |
|----------+--------|


Train on older data and test on new data

*** Learning APIs

For linear regression:

learner = LinRegLearner()

learner.train(Xtrain, Ytrain)

y = learner.query(Xtest)

These y values will be compared with Ytest values

For KNN:

learner = KNNLearner(k=3)

learner.train(Xtrain, Ytrain)

y = learner.query(Xtest)

*** Example for linear regression

#+BEGIN_SRC python

class LinRegLearner::

    def __init__():
        pass

    def train(X, Y):
        # find m and b for y = mx + b
        self.m, self.b = favorite_linreg(X, Y)         # see scipy, numpy

    def query(X):
        y = self.m * X + self.b
        return y

#+END_SRC

- https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.linregress.html
- scipy.stats.linregress


- KNN should have the same interface so you can try either type easily


- http://www.kdnuggets.com/2016/01/implementing-your-own-knn-using-python.html
- https://ashokharnal.wordpress.com/tag/k-nearest-neighbor-classification-example-using-python/
- ...

** 03-03 - Assessing a learning algorithm
*** Overview

There are many algorithms other than linear regression and knn

*** A closer look at KNN solutions

If you plot results the line is ragged.

Model can't extrapolate
Horizontal lines before and after the data

*** What happens as K varies

K = 1 just goes from one value to another
K = N just goes to the mean

As K increases are we likely to overfit? No

K=1 overfits

*** What happens as D varies

For a parameteric model
D is the number of degrees

The higher the number of polynomials (d) then the more likely to overfit.

Yes, in this case, increasing d increases model complexity, and
results in our model trying to closely align with the given data
points.

D = 1 is a linear model, results in a line
D = 2 is a parabola
D = 3 has a cubed component

*** Metric 1 RMS error

Root Mean Squared error

Error is the difference to the modeled line

RMSE = sqrt( sum( Ytest - Ypredict) ^ 2 / N )

*** In Sample vs Out of Sample

What is the out of sample?
Find the error while using the Test set

*** Quiz: Which is worse

You would expect out-of-sample error to be larger, since the model has not seen points from the test set.

*** Cross validation

|-------+-----|
| Train | 60% |
| Test  | 40% |
|-------+-----|


If not enough data
Split into multiple training sets and a single test set. Say do multiple trials where you move the test segment around

*** Roll forward cross validation

If training data is after the test data then we have looked into the future

Make sure the training data is always before the test data.

Use smaller sets and roll forward for each trial

*** Metric 2: correlation
Take XTest and YText to get YPredict

Then plot YTest and YPredict (Scatter Plot)
Is there an alignment

Measure this quantitative using correlation


numpy.corrcoef()

-1 -> 0 -> +1

*** Quiz: Correlation and RMS error

As RMS error increases, correlation goes down.

*** Overfitting

Degrees of freedom (d) (degrees of the polynomial)

Error is highest when d is the lowest
As you increase d and approach the number data points the error goes to 0

Done with Training data results in what is called 'in sample error'


If you plot the out of sample error it will decrease but will curve up
Where it curves/diverges is overfitting
- in sample is decreasing
- out of sample is increasing

*** Quiz: KNN overfitting

K from 1 to N

Error is the is the lowest when K is 1

--

When k = 1, the model fits the training data perfectly, therefore
in-sample error is low (ideally, zero).  Out-of-sample error can be
quite high.

As k increases, the model becomes more generalized, thus out-of-sample
error decreases at the cost of slightly increasing in-sample error.

After a certain point, the model becomes too general and starts
performing worse on both training and test data.

--

Overfitting happens earlier, lower k values

*** Quiz: A few other considerations

|------------------------+------------+-----|
|                        | Linear Reg | KNN |
|------------------------+------------+-----|
| Space for saving model | -          |     |
| Compute time to train  |            | -   |
| Compute time to query  | -          |     |
| Ease to add new data   |            | -   |
|------------------------+------------+-----|

** 03-04 Ensemble learners, bagging and boosting
*** Overview

1988 - "Can a set of weak learners be combined to create a stronger learner?" Kearns and Valiant .
2006 - Netflix competition
2009 - The winning algorithm was a combination of learners, an ensemble

Ensemble learners

*** Ensemble learners

KNN                            - https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm
LinReg                         - https://en.wikipedia.org/wiki/Simple_linear_regression
Decision Tree                  - https://en.wikipedia.org/wiki/Decision_tree_learning
Support Vector Machine (SVM)   - https://en.wikipedia.org/wiki/Support_vector_machine


Take the mean of the answers of all the results


Why?
- less error
- less overfitting


Each has it's own bias so when you combine the biases are reduced

*** Quiz: How to build an ensemble

If we combine several models of different types (here parameterized
polynomials and non-parameterized kNN models), we can avoid being
biased by one approach.


This typically results in less overfitting, and thus better
predictions in the long run, especially on unseen data.

*** Bootstrap aggregating bagging

Same learner but train different subsets of the data (bags of data)

Collect/bag the data randomly

Random with replacement means it is ok to grab the same data value again

| n  | number of training instances in our original data |
| n! | number of instances we put in each bag            |
| m  | number of bags                                    |


Rule of thumb

Each bag is used to train a different model

In the video (around 02:06), the professor mentions that n’ should be
set to about 60% of n, the number of training instances. It is more
accurate to say that in most implementations, n’ = n. Because the
training data is sampled with replacement, about 60% of the instances
in each bag are unique.

*** Quiz: Overfitting

A 1NN model (kNN with k = 1) matches the training data exactly, thus
overfitting.

An ensemble of such learners trained on slightly different datasets
will at least be able to provide some generalization, and typically
less out-of-sample error.

*** Bagging example

Each run looks like it is overfitting
Taking the mean of all the runs looks much more smooth

*** Boosting

AdaBoost (Adaptive Boost)

Use the training data after building the model to test the model

Build the next bag of data using the test results to weigh the choosen data according the errors found during the test run

Now, test both bags, combine outputs to build a new bag

Repeat

https://en.wikipedia.org/wiki/AdaBoost

*** Quiz: Overfitation

As m increases, AdaBoost tries to assign more and more specific data
points to subsequent learners, trying to model all the difficult
examples.

Thus, compared to simple bagging, it may result in more overfitting.

*** Summary

Boosting and bagging
- Wrappers for exisitng methods
- Hidden inside the same API. Callers don't need to know
- Reduces error
- Reduces overfitage
** 03-05 - Reinforcement learning
*** Overview

Up until this point we've focused forcast price changes and we buy the stocks with the most predicited price change.

This ignores the certainty of the price change and it doesn't help us know when to exit the position

Next,

Reinforement learners create policies which provide specific direction on which action to take

*** The RL problem

Reinforcement learning is a problem not a solution and there are a number of solutions.

Much like Linear Regression is a solution to the Supervised Learning Problem


- https://en.wikipedia.org/wiki/Reinforcement_learning

Robot example

- Sense
- Think
- Act

Reward. Take actions which maximimize the rewards

         Environment
         Learning

State -> Policy(s)      -> Action
         Lookup table

         Reward

*** Quiz: Trading as an RL Problem

|-------------------+-------+--------+--------+---|
|                   | State | Action | Reward |   |
|-------------------+-------+--------+--------+---|
| Buy               |       | X      |        |   |
| Sell              |       | X      |        |   |
| Holding Long      | X     |        |        |   |
| Bollinger Value   | X     |        |        |   |
| Return from trade |       |        | X      |   |
| Daily Return      | X     |        | X      |   |
|-------------------+-------+--------+--------+---|

*** Mapping trading to RL

The policy we learn give the directions to change state to earn a reward

Features/Holdings
Actions Buy/Sell/Do nothing
Reward/Profit

*** Markov decision problem

- Set of states (S)
- Set of actions (A)
- Transition function T[s, a, s!]
  - Cells with probabilitys that s,a -> s!
- Reward function R[s, a]

- https://en.wikipedia.org/wiki/Markov_decision_process

Find
Policy PIE[s] wthat will maximize reward


PIE* is the optimum policy

*** Unknown transitions and rewards

We don't have the PIE environment/function nor the Reward function

Experience tuble
<s1 a1 s1! r1>
<s2 a2 s2! r2>
.
.
.
<sn an sn! rn>


**** Model based reinforcment learning

Build T[a, a, s!] and R[s, a] from our list of experiences. Easy. Table-based.

Value/policy iterations

**** Model-free

Build model directly by looking at the data

*** What to optimize?

Infinite horizon
- Sum of all reward of all steps

Finite horizon
- Limit the number of steps
- Sum of reward for number of steps


Discounted reward
Gamma to i -1
Steps devalue as you increase steps
Think money over time. Interest rates
Each step in the future is worth less
Used in Q-Learning

*** Quiz: Which approach leads to 1M

Both a finite horizon of n = 10 as well as discounted rewards will
result in the robot picking a path to the $1M cell.

Infinite horizon may also lead to the robot choosing the $1M cell, but
there isn't much difference mathematically since repeatedly visiting
the $1 will also result in infinite reward.

*** Summary

RL summary

- RL algos solve Markov Decision Problems
- S, A, T[s, a, s!], R[s, a]
- Find policy PIE(s) -> a which maximizes a reward
- Map trading to RL

|---+-------------------------------------|
| S | Features/Holdings                   |
| A | Buy/sell/hold/do nothing            |
| R | Returns                             |
|---+-------------------------------------|
| T | Transitions/Market                  |
|   | ML algorithms to produce PIE/Policy |
|---+-------------------------------------|

** 03-06 - Q-Learning

*** Overview

Q-Learning
Model free approach
Does not model Transitions or Rewards


Builds a table of utility values as the agent interacts with the world
These q values can be used at each step to select the best action based on what it has learned so far.

Guaranteed to provide an optimal policy

- https://en.wikipedia.org/wiki/Q-learning

*** What is Q?

The Q function

View as a table

Q[s, a] = immediate reward + discounted reward


How to use Q?

PIE(s) = argmaxa(Q[s, a])

         step through the table of Q[s, a] where a is the value
         find the max value

         in other words, find the a which maximizes Q[s, z])

PIE*(s) = the star means optimal

         Q*[s, a]

*** Learning Procedure


Big picture
- select training data
- iterate over time <s, a, s:r>
- test policy PIE
- repeat until converg

Details
- set starttime, init Q[]
  - initial Q table with small random numbers
- compute S
- select a
- observe r, s!
  - Now have a complete experience tuple <a, a, s!, r>
- update Q

*** Update rule

Alpha is the learning rate 0 to 1.0 (usually 0.2)

Q![s, a] = (1 - Alpha) Q[s, a] + A * improved estimate

Lower Alpha, slowing learning

Gamma is the discount rate 0 to 1.0
    - low value means we value later values less

Q![s, a] = (1 - Alpha) Q[s, a] + Alpha (r + Gamaa later rewards)


                                           - future discounted rewards
Q![s, a] = (1 - Alpha) Q[s, a] + Alpha (r + Gamaa * Q[s!, argmaxa (Q[s!, a!])])

*** Update rule - notes

Update Rule

The formula for computing Q for any state-action pair <s, a>, given an experience tuple <s, a, s', r>, is:

Q'[s, a] = (1 - α) · Q[s, a] + α · (r + γ · Q[s', argmaxa'(Q[s', a'])])

Here:

- r = R[s, a] is the immediate reward for taking action a in state s,
- γ ∈ [0, 1] (gamma) is the discount factor used to progressively reduce the value of future rewards,
- s' is the resulting next state,
- argmaxa'(Q[s', a']) is the action that maximizes the Q-value among all possible actions a' from s', and,
- α ∈ [0, 1] (alpha) is the learning rate used to vary the weight given to new experiences compared with past Q-values.

*** Two finer points

- Success depends on exploration
- choose random action with probability c (say .3)
- over each iteration reduce the random actions

*** The Trading Problem - Actions

- Buy
- Sell
- Nothing

*** Quiz: The Trading Problem: Rewards

Which results in faster convergence?

r = daily return
r = 0 until exit, then cumulative return

r = daily return is correct

A reward at each step allows the learning agent get feedback on each individual action it takes (including doing nothing).

*** Quiz: The Trading Problem: State

|----------------------+---+----------------------------------------------------------------|
| Adjusted Close       |   | Not a good value because it is hard to generallize and compare |
| SMA                  |   | Same reason as Adjusted close                                  |
| Adjusted Close/SMA   | X | As a ratio is good                                             |
| Bollinger Band value | X |                                                                |
| P/E ratio            | X |                                                                |
| Holding stock        | X | Good to know if you are holding it                             |
| Return since entry   | X |                                                                |
|----------------------+---+----------------------------------------------------------------|

